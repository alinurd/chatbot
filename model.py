import json
import pickle
import nltk
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

print("="*60)
print("ğŸ¤– TRAINING CHATBOT MODEL")
print("="*60)

# Download NLTK
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    print("âœ… NLTK packages ready")
except:
    print("âš ï¸ NLTK download skipped")

# Load and check dataset
print("\nğŸ“‚ Loading dataset.json...")
try:
    with open('dataset.json', 'r', encoding='utf-8') as f:
        content = f.read()
        print(f"File size: {len(content)} characters")
        print(f"First 200 chars: {content[:200]}...")
        
        # Parse JSON
        data = json.loads(content)
        print(f"âœ… JSON parsed successfully")
        print(f"Type: {type(data)}")
        
        if isinstance(data, list):
            print(f"ğŸ“Š Dataset: {len(data)} items")
            print(f"First item: {data[0]}")
        elif isinstance(data, dict):
            print(f"ğŸ“Š Dataset: dictionary with keys: {list(data.keys())}")
        else:
            print(f"â“ Unknown data type: {type(data)}")
            
except json.JSONDecodeError as e:
    print(f"âŒ JSON Error: {e}")
    print("ğŸ’¡ Periksa format JSON!")
    exit(1)
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Jika data adalah dictionary, konversi ke list
if isinstance(data, dict):
    print("\nâš ï¸  Dataset is a dictionary, converting to list...")
    if 'intents' in data:
        data = data['intents']
    else:
        # Assume it's already in the right format but wrapped
        data = list(data.values())
    
print(f"\nğŸ“Š Final dataset: {len(data)} intents")

# Preprocessing
stop_words = set(stopwords.words('indonesian'))

def preprocess(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Prepare data
patterns = []
tags = []
responses = {}

print("\nğŸ”§ Processing intents...")
for i, item in enumerate(data):
    try:
        if isinstance(item, dict):
            tag = item.get('tag', f'intent_{i}')
            patterns_list = item.get('patterns', [])
            responses_list = item.get('responses', [])
            
            responses[tag] = responses_list
            
            for pattern in patterns_list:
                patterns.append(preprocess(pattern))
                tags.append(tag)
                
            print(f"   {i+1:2d}. {tag}: {len(patterns_list)} patterns")
        else:
            print(f"   âš ï¸  Item {i} is not a dict: {type(item)}")
    except Exception as e:
        print(f"   âŒ Error processing item {i}: {e}")

print(f"\nğŸ“ˆ Data summary:")
print(f"   Total patterns: {len(patterns)}")
print(f"   Unique tags: {len(set(tags))}")
print(f"   Tags: {', '.join(sorted(set(tags)))}")

if len(patterns) == 0:
    print("âŒ Tidak ada data untuk training!")
    exit(1)

# Vectorization
print("\nğŸ”¢ Vectorizing...")
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(patterns)
y = np.array(tags)

print(f"   Vocabulary size: {len(vectorizer.vocabulary_)}")

# Train model
print("\nğŸ¤– Training Naive Bayes model...")
model = MultinomialNB()
model.fit(X, y)

# Training accuracy
train_accuracy = model.score(X, y) * 100
print(f"âœ… Training accuracy: {train_accuracy:.2f}%")

# Save model
print("\nğŸ’¾ Saving model files...")
with open('chatbot_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

with open('responses.pkl', 'wb') as f:
    pickle.dump(responses, f)

print("\n" + "="*60)
print("ğŸ‰ TRAINING COMPLETE!")
print("="*60)
print("ğŸ“ Files saved:")
print("   â€¢ chatbot_model.pkl")
print("   â€¢ vectorizer.pkl")
print("   â€¢ responses.pkl")
print(f"\nğŸ“Š Model info:")
print(f"   â€¢ Training samples: {len(patterns)}")
print(f"   â€¢ Intents: {len(responses)}")
print(f"   â€¢ Accuracy: {train_accuracy:.2f}%")
print("\nğŸš€ Start API with: python app.py")
print("="*60)